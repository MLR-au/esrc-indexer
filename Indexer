#!/usr/local/scholarly-python2/bin/python

import sys
import argparse
import logging
import ConfigParser
import os.path
from indexer.Indexer import Indexer
from indexer.Timer import Timer
from indexer.Crawler import Crawler
from indexer.Transformer import Transformer
from indexer.Poster import Poster

if __name__ == "__main__":

    # read and check the options
    parser = argparse.ArgumentParser(description='eSRC Indexer')

    parser.add_argument('--config',   dest='config', required=True, help='The path to the default Indexer configuration.')
    parser.add_argument('--site',     dest='site',   required=True, help='The site to process.')

    parser.add_argument('--crawl', dest='crawl', action='store_true', default=None,
        help='Only perform the crawl and transform stages.')
    parser.add_argument('--post', dest='post', action='store_true', default=None,
        help='Only perform the post stage (includes index clean).')

    parser.add_argument('--clean', dest='clean', action='store_true', default=None,
        help="Wipe this site's data from the index first or not?")

    parser.add_argument('--process-hdms-data', dest='ead_data', action='store_true', default=None,
        help='Process a HDMS EAD data file.')

    parser.add_argument('--solr', dest='solr', default=None,
        help='Override the solr defined in the config file.')

    parser.add_argument('--info', dest='info', action='store_true', help='Turn on informational messages')
    parser.add_argument('--debug', dest='debug', action='store_true', help='Turn on full debugging (includes --info)')

    args = parser.parse_args()

    # unless we specify otherwise
    if args.debug:
        logging.basicConfig(level=logging.DEBUG)

    if args.info:
        logging.basicConfig(level=logging.INFO)

    if not (args.debug and args.info):
        # just give us error messages
        logging.basicConfig(level=logging.WARN)

    # get the logger
    log = logging.getLogger('INDEXER')

    # check the arguments
    if not os.path.exists(args.config):
        log.error("Does %s exist?" % args.config)
        sys.exit()

    log.debug("Indexing: %s" % args.site)

    indexer = Indexer(args.site, args.config)

    if args.crawl is not None:
        ### CRAWLER
        content = indexer.crawl()

        ### TRANSFORMER
        ### Only do the crawl if the user is not requesting to specifically
        ###  process a single file; this is usually testing
        indexer.transform(content)

    if args.ead_data is not None:
        ### EAD Processor
        indexer.process_hdms_data()

    if args.post is not None:
        ### POSTER
        indexer.post(args.solr, args.clean)
    
