#!/usr/local/scholarly-python2/bin/python

import sys
import argparse
import logging
import ConfigParser
import os.path
from indexer.Timer import Timer
from indexer.Crawler import Crawler
from indexer.Transformer import Transformer
from indexer.Poster import Poster

class Indexer:
    def __init__(self, site):
        # get the default configuration
        cfg = ConfigParser.SafeConfigParser()
        cfg.read(args.config)

        self.site = site
        site_configuration = os.path.join(self.config_get(cfg, 'GENERAL', 'configs'), site)
        self.site_cache = os.path.join(self.config_get(cfg, 'GENERAL', 'cache_path'), site)
        self.default_transforms = self.config_get(cfg, 'GENERAL', 'transforms')
        log.debug("Configuration: %s" % site_configuration)
        log.debug("Cache path: %s" % self.site_cache)

        # then try to load the site specific config
        if not os.path.exists(site_configuration):
            log.error("Can't access %s" % site_configuration)
            sys.exit()

        # read in the site specific configuration and kick off the run
        cfg.read(site_configuration)
        self.cfg = cfg

    def crawl(self):
        ### CRAWL THE SOURCE FOLDER
        files_list = []

        input_folder = self.config_get(self.cfg, 'crawl', 'input')
        excludes = self.config_get(self.cfg, 'crawl', 'excludes')
        source = self.config_get(self.cfg, 'crawl', 'source').split(',')
        log.debug("Input folder for crawl: %s" % input_folder)
        log.debug("Excludes list: %s" % excludes)
        log.debug("Map to source: %s" % source)

        if input_folder is None:
            log.error("I think input folder is missing from the config: %s" % site_configuration)
            sys.exit()

        with Timer() as t:
            c = Crawler(input_folder, excludes, source)
            files_list = c.run()

        return files_list

    def transform(self, content, document=None, doctype=None):
        ### TRANSFORM THE CONTENT MARKED FOR INGESTION INTO SOLR
        output_folder = os.path.join(self.site_cache, 'post')
        transforms = self.config_get(self.cfg, 'transform', 'transforms')

        if not transforms:
            transforms = self.default_transforms
        else:
            transforms = [ transforms, self.default_transforms ]

        log.debug("Output folder for transforms: %s" % output_folder)
        log.debug("Transform search path: %s" % transforms)

        with Timer() as t:
            t = Transformer(content, self.site, output_folder, transforms)
            if document is not None:
                t.process_document((document, doctype), debug=True)
            else:
                t.run()

    def post(self):
        ### POST THE SOLR DOCUMENTS TO THE INDEX
        input_folder = os.path.join(self.site_cache, 'post')
        solr_service = self.config_get(self.cfg, 'post', 'index')

        log.debug("Content folder to be posted : %s" % input_folder)
        log.debug("Solr service: %s" % solr_service)

        with Timer() as t:
            p = Poster(input_folder, solr_service, self.site)
            p.run()

    def config_get(self, cfg, section, param):

        if cfg.has_section(section) and cfg.has_option(section, param):
            return cfg.get(section, param)
        else:
            return None


if __name__ == "__main__":

    # read and check the options
    parser = argparse.ArgumentParser(description='eSRC Indexer')

    parser.add_argument('--config',   dest='config', required=True, help='The path to the default Indexer configuration.')
    parser.add_argument('--site',     dest='site',   required=True, help='The site to process.')

    parser.add_argument('--post-only', dest='post', action='store_true', default=None,
        help='Only perform the post stage (includes index clean).')

    parser.add_argument('--transform-document', dest='transform_document', default=False,
        help='Transform a single document and write the result to STDOUT.')
    parser.add_argument('--transform-document-type', dest='transform_document_type', choices = ['xml', 'html'], default=None,
        help='Transform a single document and write the result to STDOUT.')
    parser.add_argument('--post-document', dest='post_document', default=None,
        help='Post this document to SOLR.')

    parser.add_argument('--info', dest='info', action='store_true', help='Turn on informational messages')
    parser.add_argument('--debug', dest='debug', action='store_true', help='Turn on full debugging (includes --info)')

    args = parser.parse_args()

    # unless we specify otherwise
    if args.debug:
        logging.basicConfig(level=logging.DEBUG)

    if args.info:
        logging.basicConfig(level=logging.INFO)

    if not (args.debug and args.info):
        # just give us error messages
        logging.basicConfig(level=logging.ERROR)

    # get the logger
    log = logging.getLogger('INDEXER')

    # check the arguments
    if not os.path.exists(args.config):
        log.error("Does %s exist?" % args.config)
        sys.exit()

    if args.transform_document and not args.transform_document_type:
        log.error("If you specify a doc to transform via --transform-document, you must also state its type via --transform-document-type")
        sys.exit()

    log.debug("Indexing: %s" % args.site)


    indexer = Indexer(args.site)

    if args.post is False:
        ### CRAWLER
        content = indexer.crawl()

        ### TRANSFORMER
        ### Only do the crawl if the user is not requesting to specifically
        ###  process a single file; this is usually testing
        if args.transform_document is None:
            indexer.transform(content)
        else:
            indexer.transform(content, document=args.document, doctype=args.document_type)

    ### POSTER
    indexer.post()




